{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7984f5b7",
   "metadata": {},
   "source": [
    "# nanoTabStar: Manual Inference & Inspection\n",
    "\n",
    "This notebook demonstrates how to load a trained **nanoTabStar** model and perform manual predictions on samples from the pretrain corpus. This is useful for inspecting how the model interprets specific feature combinations and target descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eabb6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add project root to path to import nanotabstar\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from nanotabstar.model import TabSTARModel\n",
    "from nanotabstar.data_loader import TabSTARDataLoader\n",
    "from nanotabstar.metrics import calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c77309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "H5_PATH = \"../data/pretrain_corpus_tabstar.h5\"\n",
    "MODEL_PATH = \"../best_model.pt\"\n",
    "MODEL_NAME = \"intfloat/e5-small-v2\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf4a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from ../best_model.pt...\n",
      "Model ready for inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2. Initialize Model Architecture\n",
    "model = TabSTARModel(d_model=384, n_layers=6, n_heads=6)\n",
    "\n",
    "# 3. Load Weights\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"Loading weights from {MODEL_PATH}...\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "else:\n",
    "    print(f\"WARNING: {MODEL_PATH} not found. Using a fresh model (random weights).\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4e5f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabSTARDataLoader (val) initialized with 5 datasets.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Dataloader on the validation split\n",
    "# We use a small batch size for manual inspection\n",
    "data_loader = TabSTARDataLoader(\n",
    "    h5_path=H5_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=10,\n",
    "    steps_per_epoch=1,\n",
    "    split='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5897387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(batch):\n",
    "    feat_ids = batch[\"feature_input_ids\"].to(DEVICE)\n",
    "    feat_mask = batch[\"feature_attention_mask\"].to(DEVICE)\n",
    "    feat_nums = batch[\"feature_num_values\"].to(DEVICE)\n",
    "    target_ids = batch[\"target_token_ids\"].to(DEVICE)\n",
    "    target_mask = batch[\"target_attention_mask\"].to(DEVICE)\n",
    "    task_type = batch[\"task_type\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            feature_input_ids=feat_ids,\n",
    "            feature_attention_mask=feat_mask,\n",
    "            feature_num_values=feat_nums,\n",
    "            target_token_ids=target_ids,\n",
    "            target_attention_mask=target_mask,\n",
    "            task_type=task_type\n",
    "        )\n",
    "    \n",
    "    return logits, batch[\"labels\"], task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e259df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset: CONCRETE_STRENGTH (regression) ---\n",
      "\n",
      "Sample 1:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: -0.3951\n",
      "  Prediction:   -0.5467\n",
      "\n",
      "Sample 2:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: -0.1250\n",
      "  Prediction:   -1.0056\n",
      "\n",
      "Sample 3:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: 1.0685\n",
      "  Prediction:   -0.4397\n",
      "\n",
      "Sample 4:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: -0.4718\n",
      "  Prediction:   -0.7448\n",
      "\n",
      "Sample 5:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: 0.4643\n",
      "  Prediction:   -0.6659\n",
      "\n",
      "Sample 6:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: -0.0035\n",
      "  Prediction:   -0.1680\n",
      "\n",
      "Sample 7:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: -1.3354\n",
      "  Prediction:   -1.2445\n",
      "\n",
      "Sample 8:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: -0.3646\n",
      "  Prediction:   -0.7703\n",
      "\n",
      "Sample 9:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: 0.3427\n",
      "  Prediction:   -0.1012\n",
      "\n",
      "Sample 10:\n",
      "  Target Description: CONCRETE_STRENGTH prediction\n",
      "  Ground Truth: 0.1259\n",
      "  Prediction:   -0.5417\n"
     ]
    }
   ],
   "source": [
    "# Fetch one batch and inspect\n",
    "for batch in data_loader:\n",
    "    logits, labels, task_type = predict_batch(batch)\n",
    "    \n",
    "    print(f\"--- Dataset: {batch['dataset_name']} ({task_type}) ---\")\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        \n",
    "        # 1. Show some features (first 3 for brevity)\n",
    "        # Note: We don't have the raw strings here easily, but we can see the task\n",
    "        print(f\"  Target Description: {batch['dataset_name']} prediction\")\n",
    "        \n",
    "        # 2. Show Ground Truth vs Prediction\n",
    "        true_val = labels[i].item()\n",
    "        \n",
    "        if task_type == 'classification':\n",
    "            probs = torch.softmax(logits[i], dim=0)\n",
    "            pred_class = torch.argmax(probs).item()\n",
    "            conf = probs[pred_class].item()\n",
    "            print(f\"  Ground Truth: Class {true_val}\")\n",
    "            print(f\"  Prediction:   Class {pred_class} (Conf: {conf:.2%})\")\n",
    "        else:\n",
    "            pred_val = logits[i].item()\n",
    "            print(f\"  Ground Truth: {true_val:.4f}\")\n",
    "            print(f\"  Prediction:   {pred_val:.4f}\")\n",
    "    break # Only one batch for inspection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
